<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dominique Makowski">
<meta name="author" content="Ana Neves">
<meta name="keywords" content="illusion sensitivity, visual illusions, phenomenological control, suggestibility, hypnotizability">
<meta name="description" content="ILLUSION SENSITIVITY AND PHENOMENOLOGICAL CONTROL">

<title>Testing the Relationship between Phenomenological Control related to Illusion Sensitivity</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="manuscript_files/libs/clipboard/clipboard.min.js"></script>
<script src="manuscript_files/libs/quarto-html/quarto.js"></script>
<script src="manuscript_files/libs/quarto-html/popper.min.js"></script>
<script src="manuscript_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="manuscript_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="manuscript_files/libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="manuscript_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="manuscript_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="manuscript_files/libs/bootstrap/bootstrap-d02e3dd41d8954d66fc9be5972330ef2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="manuscript_files/libs/tabwid-1.1.3/tabwid.css" rel="stylesheet">

<script src="manuscript_files/libs/tabwid-1.1.3/tabwid.js"></script>



<link rel="stylesheet" href="_extensions/wjschne/apaquarto/apa.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#methods" id="toc-methods" class="nav-link active" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#participants" id="toc-participants" class="nav-link" data-scroll-target="#participants">Participants</a></li>
  <li><a href="#procedure" id="toc-procedure" class="nav-link" data-scroll-target="#procedure">Procedure</a>
  <ul class="collapse">
  <li><a href="#phenomenological-control-scale-pcs" id="toc-phenomenological-control-scale-pcs" class="nav-link" data-scroll-target="#phenomenological-control-scale-pcs">Phenomenological Control Scale (PCS)</a></li>
  <li><a href="#illusion-game" id="toc-illusion-game" class="nav-link" data-scroll-target="#illusion-game">Illusion Game</a></li>
  </ul></li>
  <li><a href="#data-analysis" id="toc-data-analysis" class="nav-link" data-scroll-target="#data-analysis">Data Analysis</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#data-availability" id="toc-data-availability" class="nav-link" data-scroll-target="#data-availability">Data Availability</a></li>
  <li><a href="#acknowledgments" id="toc-acknowledgments" class="nav-link" data-scroll-target="#acknowledgments">Acknowledgments</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="manuscript.docx"><i class="bi bi-file-word"></i>MS Word (apaquarto)</a></li><li><a href="manuscript.pdf"><i class="bi bi-file-pdf"></i>PDF (apaquarto)</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content">




<br>

<br>

<section id="title" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Testing the Relationship between Phenomenological Control related to Illusion Sensitivity</h1>
<div class="Author">
<br>

<p>Dominique Makowski and Ana Neves</p>
<p>School of Psychology, University of Sussex</p>
</div>
</section>
<section id="author-note" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Author Note</h1>
<p>Dominique Makowski <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" width="16" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0001-5375-9967</p>
<p>Ana Neves <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" width="16" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0009-0006-0020-7599</p>
<p>Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows: <em>Dominique Makowski</em><strong>: </strong>conceptualization, data curation, formal analysis, funding acquisition, investigation, methodology, project administration, resources, software, supervision, validation, visualization, writing – original draft, and writing – review &amp; editing. <em>Ana Neves</em><strong>: </strong>project administration, data curation, formal analysis, investigation, visualization, writing – original draft, and writing – review &amp; editing</p>
<p>Correspondence concerning this article should be addressed to Dominique Makowski, School of Psychology, University of Sussex, Email: D.Makowski@sussex.ac.uk</p>
</section>
<section id="abstract" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Abstract</h1>
<div class="Abstract">
<p>Visual illusions highlight how easily our conscious experience can be altered with respect to perceptual reality. Despite sharing in-principle mechanisms with phenomenological control, i.e., the ability to alter our perceptual experience to match task demands or expectations, research tying the two remains scarce. This study aims to replicate and expand Lush et al.&nbsp;(2022) reporting an absence of correlation between phenomenological control (measured using the Phenomenological Control Scale) and illusion sensitivity to different illusion types. <em>[N participants were recruited in an online study. Results will be added in the final version of the manuscript]</em>.</p>
</div>
<p><em>Keywords</em>: illusion sensitivity, visual illusions, phenomenological control, suggestibility, hypnotizability</p>
</section>
<section id="firstheader" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Testing the Relationship between Phenomenological Control related to Illusion Sensitivity</h1>
<p>Visual Illusions are an interesting type of stimuli highlighting the ease with which our phenomenological conscious experience can become dissociated from physical reality. Their robust and reliable effect makes them useful stimuli to explore how perception is constructed and shaped, and several theoretical models have been put forth to explain how they work. <strong>In particular, illusions have been reframed using a predictive coding account of perception <span class="citation" data-cites="notredame2014 nour2015perception">(e.g., <a href="#ref-notredame2014" role="doc-biblioref">Notredame et al., 2014</a>; <a href="#ref-nour2015perception" role="doc-biblioref">Nour &amp; Nour, 2015</a>)</span></strong> in which the brain optimally combines, using some flavour of Bayesian inference, perceptual inputs with prior knowledge to make sense of ambiguous environments <span class="citation" data-cites="friston2010">(<a href="#ref-friston2010" role="doc-biblioref">Friston, 2010</a>)</span>.</p>
<p>Such computational model(s) propose to conceptualize illusions as stimuli providing weak or conflicting sensory evidence <span class="citation" data-cites="sundareswara2008 gershman2012">(<a href="#ref-gershman2012" role="doc-biblioref">Gershman et al., 2012</a>; <a href="#ref-sundareswara2008" role="doc-biblioref">Sundareswara &amp; Schrater, 2008</a>)</span> that bias perception toward prior knowledge. In other words, the weight of priors, in the form of perceptual knowledge about the world (e.g., internalized rules of perspective) is amplified when the sensory input is confusing. For instance, in the Müller-Lyer illusion, we “compute” the two (actually identical) lines as being of different lengths because the line flanked with converging fins is misinterpreted as being further away <span class="citation" data-cites="notredame2014">(<a href="#ref-notredame2014" role="doc-biblioref">Notredame et al., 2014</a>)</span>. In this context, measuring sensitivity to illusion can be operationalized as indexing the parameters of the Bayesian inference process (e.g., prior precision).</p>
<p>These accounts also provide a compelling framework to explain existing findings reporting interindividual variability in the sensitivity to illusions. Indeed, several studies suggest a potential link with psychopathology, in particular schizophrenia <span class="citation" data-cites="costa2023">(<a href="#ref-costa2023" role="doc-biblioref">Costa et al., 2023</a>)</span> and autism <span class="citation" data-cites="gori2016">(<a href="#ref-gori2016" role="doc-biblioref">Gori et al., 2016</a>)</span>, in which the reported lower sensitivity to illusions has been attributed to a diminished influence of top-down processes such as prior knowledge <span class="citation" data-cites="mitchell2010">(<a href="#ref-mitchell2010" role="doc-biblioref">Mitchell et al., 2010</a>)</span> and a greater emphasis on (i.e., precision of) sensory information <span class="citation" data-cites="palmer2017">(<a href="#ref-palmer2017" role="doc-biblioref">Palmer et al., 2017</a>)</span>. Evidence beyond psychopathology also suggests variability in the general population, potentially correlated with personality traits such as agreeableness and honest-humility <span class="citation" data-cites="makowski2023">(<a href="#ref-makowski2023" role="doc-biblioref">Makowski et al., 2023</a>)</span>, as well as cognitive abilities <span class="citation" data-cites="shoshina2014">(<a href="#ref-shoshina2014" role="doc-biblioref">Shoshina &amp; Shelepin, 2014</a>)</span>.</p>
<p>However, the exact nature of this interindividual variability and its potential origin remains unclear. The somewhat mixed evidence in the literature regarding its generalizability and strength could be related to the variety of the paradigms used and the type of processes being mobilised <span class="citation" data-cites="makowski2021">(<a href="#ref-makowski2021" role="doc-biblioref">Makowski et al., 2021</a>)</span>. Indeed, traditional methods frequently focus on participant’s experience by prompting them to assess the difference between two identical targets, estimate the target’s physical properties, or adjust the targets to match a reference stimulus <span class="citation" data-cites="todorovic2020">(<a href="#ref-todorovic2020" role="doc-biblioref">Todorović, 2020</a>)</span>. Relying on metacognitive judgments about one’s subjective experiences adds an additional layer to the measure that might not be desired when attempting to measure illusion <strong>sensitivity</strong>. Moreover, paradigms often face challenges in diversifying the illusory effects (i.e., using multiple stimuli to experimentally manipulate the strength of the illusion) and the illusion types (i.e., using various illusions, such as Müller-Lyer, Ebbinghaus, Delboeuf which might rely on a different admixture of mechanisms), hindering the potential of obtaining a comprehensive, valid, and reliable measure of illusion sensitivity.</p>
<p>The “Illusion Game” paradigm <span class="citation" data-cites="makowski2023">(<a href="#ref-makowski2023" role="doc-biblioref">Makowski et al., 2023</a>)</span> has been recently developed to measure illusion sensitivity to various illusion types through its behavioural impact (on response time and error rate) in a perceptual decision task (where participants have to respond as fast as possible; e.g., “which of the left or right circles is bigger”). The stimuli for different classical illusions are created using the <em>Pyllusion</em> software <span class="citation" data-cites="makowski2021">(<a href="#ref-makowski2021" role="doc-biblioref">Makowski et al., 2021</a>)</span>, which allows researchers to modulate the strength of the illusion as a continuous dimension, independently from the difficulty of the perceptual task. This paradigm, inspired by psychophysics, lends itself to the computational modelling of illusion sensitivity through its <strong>interference effect —an effect that disrupts an individual’s ability to accurately discriminate between perceptual stimuli.</strong> <strong>This approach aims to bypass some of the metacognitive processes involved in other paradigms, offering a more direct and objective measure of how illusions influence perceptual judgment.</strong></p>
<p>Interestingly, the fact that inter-individual variability in illusion sensitivity seems to persist in this task suggests that it is not solely explained by <strong>metacognitive ability differences</strong>, and gives rise to the following question: is the variability in illusion sensitivity related to low-level perceptual processes (e.g., baseline precision of perceptual priors), or rather to the ability to actively control and “resist” the illusion in order to achieve the task at hand (higher-level modulation of the perceptual inference parameters). If the latter is true, then illusion sensitivity measured in contexts with strong task-demand characteristics, e.g., in paradigms where participants’ performance is explicitly or implicitly assessed (i.e., where there is an incentive to downplay the illusion effect) might correlate with one’s ability to alter one’s subjective experience following suggestions - a mechanism referred to as “phenomenological control”.</p>
<p>The idea that we are endowed with the potential to unconsciously alter our subjective experience and distort reality - even momentarily - to meet the goals at hand is not novel. While this phenomenon has been historically often studied under the label of “hypnotisability” - the tendency to alter our conscious experience to match external demands <span class="citation" data-cites="lush2021">(<a href="#ref-lush2021" role="doc-biblioref">Lush et al., 2021</a>)</span>, the term “phenomenological control” (PC) has been recently introduced to disconnect this concept from the potentially negative associations with hypnosis and the misconception that a hypnotic context is necessary for responding to imaginative suggestions <span class="citation" data-cites="dienes2022">(<a href="#ref-dienes2022" role="doc-biblioref">Dienes et al., 2022</a>)</span>.</p>
<p>To encourage the empirical exploration of our ability and tendency to alter our phenomenological experience and further accelerate investigations away from the hypnotic context, Lush et al. (<a href="#ref-lush2021">2021</a>) adapted the Sussex-Waterloo Scale of Hypnotisability <span class="citation" data-cites="lush2018">(SWASH, <a href="#ref-lush2018" role="doc-biblioref">Lush et al., 2018</a>)</span> by removing all its references to hypnosis, to measure trait phenomenological control. <strong>This newly developed phenomenological control scale (PCS) consists of 10 imaginative suggestions followed by subjective ratings for each suggestion and has demonstrated validity in online experiments <span class="citation" data-cites="lush2022">(<a href="#ref-lush2022" role="doc-biblioref">Lush et al., 2022</a>)</span>.</strong></p>
<p>Interestingly, Lush et al. (<a href="#ref-lush2022">2022</a>) did test for a relationship between PC and illusion sensitivity using the Müller-Lyer illusion (in which the arrangement of the arrowheads flanking two lines makes them appear as having different lengths), and reported evidence in favour of an absence of correlation between the two measures. This finding was interpreted as indicative of the cognitive impenetrability of illusions, implying that the effect is driven by low-level processes and therefore not influenced by top-down mechanisms such as PC. <strong>Note that both prior-knowledge and phenomenological control are considered top-down processes, but the cognitive impenetrability hypothesis suggests that the processes at stake for the illusions happen at a lower- encapsulated- level (in the form of <em>perceptual</em> priors)</strong>.</p>
<p>The goal of this study is thus to replicate the results from Lush et al. (<a href="#ref-lush2022">2022</a>) pointing to an absence of a relationship between phenomenological control and illusion sensitivity, by generalising them to a different illusion paradigm that encompasses other illusion types. <strong>Additionally, we will explore the relationship between psychoticism, as a proxy for schizophrenia, and illusion sensitivity to assess the potential impact of lower-level effects—such as weak priors observed in individuals with schizophrenia <span class="citation" data-cites="costa2023">(<a href="#ref-costa2023" role="doc-biblioref">Costa et al., 2023</a>)</span>—on sensitivity to illusions.</strong> <strong>These analyses may offer evidence clarifying whether inter-individual variability in illusion sensitivity is driven by lower-level perceptual mechanisms or higher-level cognitive processes (<a href="#tbl-DesignTable" class="quarto-xref" aria-expanded="false">Table&nbsp;1</a>).</strong></p>
<div class="cell FigureWithoutNote" prefix="" data-tblnum="1" data-custom-style="FigureWithoutNote">
<div id="tbl-DesignTable" class="cell quarto-float quarto-figure quarto-figure-center" prefix="" data-tblnum="1">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-DesignTable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Table&nbsp;1</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Study Design Table</p>
</div>
</figcaption>
<div aria-describedby="tbl-DesignTable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div class="tabwid"><style>.cl-0bf2efb0{table-layout:auto;width:100%;}.cl-0ba57320{font-family:'Times New Roman';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0ba57352{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0bba999e{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 2;background-color:transparent;}.cl-0bbcffb8{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0.75pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0bbcffcc{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0bbcffd6{background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing="true" class="cl-0bf2efb0"><tbody><tr style="overflow-wrap:break-word;"><td class="cl-0bbcffb8"><p class="cl-0bba999e"><span class="cl-0ba57320">Question</span></p></td><td class="cl-0bbcffb8"><p class="cl-0bba999e"><span class="cl-0ba57352">Is there a correlation between trait phenomenological control (PC) and visual illusion (VI) sensitivity? Additionally, is there a relationship between VI sensitivity and the psychoticism facet of the PID-5, as a proxy for schizophrenia-related traits?</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0bbcffcc"><p class="cl-0bba999e"><span class="cl-0ba57320">Hypothesis</span></p></td><td class="cl-0bbcffcc"><p class="cl-0bba999e"><span class="cl-0ba57352">In line with Lush et al. (2022), we hypothesise that there will be evidence supporting the absence of a relationship between PC and VI sensitivity. Based on Makowski et al. (2023) and prior work on weak priors in schizophrenia, we hypothesise that higher psychoticism scores will be positively associated with VI sensitivity.</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0bbcffcc"><p class="cl-0bba999e"><span class="cl-0ba57320">Sampling Plan</span></p></td><td class="cl-0bbcffcc"><p class="cl-0bba999e"><span class="cl-0ba57352">The goal is to recruit around 500 adult English speakers using Prolific. This sample size is based on the ones used in Lush et al., 2021 and Lush et al., 2022 that we aim at replicate.</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0bbcffcc"><p class="cl-0bba999e"><span class="cl-0ba57320">Analysis Plan</span></p></td><td class="cl-0bbcffcc"><p class="cl-0bba999e"><span class="cl-0ba57352">Bayesian correlations will be conducted using the BayesFactor::correlationBF() function, with a medium prior (r-scale = 1/3), separately for: 1) PC scores and VI sensitivity (error rate and IES), across all three illusion types; and 2) Psychoticism facet scores from the PID-5 and VI sensitivity scores, across all three illusion types.</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0bbcffcc"><p class="cl-0bba999e"><span class="cl-0ba57320">Rationale for Deciding the Sensitivity of the Test</span></p></td><td class="cl-0bbcffcc"><p class="cl-0bba999e"><span class="cl-0ba57352">For the PC–VI sensitivity relationship, we will interpret BF₁₀ ≤ 1/3 as evidence against a relationship, in line with Lush et al. (2022). For the psychoticism–VI sensitivity relationship, BF₁₀ &gt; 3 will be interpreted as evidence supporting a relationship, following findings by Makowski et al. (2023).</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0bbcffcc"><p class="cl-0bba999e"><span class="cl-0ba57320">Interpretation Given Different Outcomes</span></p></td><td class="cl-0bbcffcc"><p class="cl-0bba999e"><span class="cl-0ba57352">If there is no evidence for a PC–VI relationship across all three illusions, it would support the hypothesis that VI sensitivity is independent from PC. If a positive association is found between psychoticism and VI sensitivity, it may suggest a low-level perceptual basis for inter-individual differences in illusion sensitivity.</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-0bbcffd6"><p class="cl-0bba999e"><span class="cl-0ba57320">Theory That Could Be Shown Wrong by the Outcomes</span></p></td><td class="cl-0bbcffd6"><p class="cl-0bba999e"><span class="cl-0ba57352">The cognitive impenetrability of visual illusions, which posits that illusion sensitivity is driven solely by low-level processes and is not influenced by top-down mechanisms such as phenomenological control. Conversely, a lack of association with psychoticism would challenge the view that low-level perceptual alterations underlie illusion sensitivity in non-clinical populations.</span></p></td></tr></tbody></table></div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<section id="participants" class="level2">
<h2 data-anchor-id="participants">Participants</h2>
<p>We aim to recruit around 500 <span class="citation" data-cites="lush2021 lush2022">(in line with the sample sizes used in <a href="#ref-lush2021" role="doc-biblioref">Lush et al., 2021</a>; <a href="#ref-lush2022" role="doc-biblioref">Lush et al., 2022</a>)</span> adult English native speakers with a desktop device using Prolific (www.prolific.co). Participants will be first presented with an explanatory statement and the consent form, and can proceed by pressing a button to confirm they have read and understood the information. This study has been approved by the ethics board of the School of Psychology of the University of Sussex (ER/ASF25/5).</p>
</section>
<section id="procedure" class="level2">
<h2 data-anchor-id="procedure">Procedure</h2>
<p>The experiment’s setup follows of the born-open principle <span class="citation" data-cites="deleeuw2023">(<a href="#ref-deleeuw2023" role="doc-biblioref">De Leeuw, 2023</a>)</span>. The online experiment, implemented entirely using JsPsych <span class="citation" data-cites="de2015jspsych">(<a href="#ref-de2015jspsych" role="doc-biblioref">De Leeuw, 2015</a>)</span>, has its code stored on GitHub and will leverage the power of the platform to host the experiment for free. Participant’s raw data files (containing identifiers) <strong>are</strong> automatically stored in a private OSF repository. The preprocessing and analysis scripts, as well as the anonymized data, will be available directly on GitHub, ensuring the transparency and reproducibility of all the analysis steps.</p>
<p>Participants will be presented with a consent form followed by demographic questions (gender, education level, age, and ethnicity). <strong>Although these variables are not directly analyzed in the current study, they will be used to provide a detailed and thorough description of the sample and maximizing data reusability.</strong> <strong>Participants will then be administered the PCS and the Illusion Game task (IG) in a counterbalanced order.</strong></p>
<section id="phenomenological-control-scale-pcs" class="level3">
<h3 data-anchor-id="phenomenological-control-scale-pcs">Phenomenological Control Scale (PCS)</h3>
<p>Participants will be asked to put on their headphones and await further auditory instructions. The PCS procedure starts with a recorded introduction explaining that a series of tests will be applied to evaluate how experiences can be created through imagination. This will be followed by 10 suggestions in a fixed order <span class="citation" data-cites="lush2021">(see <a href="#ref-lush2021" role="doc-biblioref">Lush et al., 2021</a>)</span>, such as “now extend your arms ahead of you, with palms facing each other, hands about a foot apart” and “as you sit comfortably in your chair with your eyes closed, a picture of two balls will be displayed on the computer screen”. <strong>Once the 10 suggestions are completed, participants will be asked to rate their subjective experiences and response to each suggestion on a 6-points Likert scale (from 0-5).</strong> Phenomenological control will be indexed by averaging the scores from the 10 scales.</p>
</section>
<section id="illusion-game" class="level3">
<h3 data-anchor-id="illusion-game">Illusion Game</h3>
<p>The task is an adaptation of the one used in Makowski et al. (<a href="#ref-makowski2023">2023</a>) to make it shorter, in which participants must make perceptual judgments (e.g., “which red line is the longer”) as quickly and accurately as possible. It includes 3 illusion types, namely Ebbinghaus, Müller-Lyer, and Vertical-Horizontal (see <a href="#fig-illusionexample" class="quarto-xref" aria-expanded="false">Figure&nbsp;1</a>). <strong>In the original Illusion Game, 10 visual illusions were presented in two sets, following a practice trial, and separated by two short questionnaires.</strong> <strong>Participants completed a total of 1,340 trials, with the experiment lasting approximately 55 minutes.</strong> <strong>In the current procedure, only three illusions are used, selected based on the original study’s findings that these illusions most strongly contribute to illusion sensitivity.</strong></p>
<div class="cell FigureWithoutNote" data-apa-twocolumn="false" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-illusionexample" class="quarto-float quarto-figure quarto-figure-center" prefix="" data-fignum="1">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-illusionexample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;1</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>The study involved three visual illusions, in which participants were instructed to respond as quickly as possible without making errors. Each illusion included two manipulated parameters: strength (e.g., the angle of the outward- or inward-pointing arrow-like fins in the Müller-Lyer illusion) and difficulty (e.g., the difference in line lengths in the Müller-Lyer illusion).</p>
</div>
</figcaption>
<div aria-describedby="fig-illusionexample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/IllusionTable.jpg" class="img-fluid figure-img" style="width:100.0%">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>The procedure encompasses 2 sets of 80 trials for each illusion type, <strong>preceded by a practice trial for each illusion</strong>. Each set will include, in a random order, the 3 blocks of illusion types, in which trials are separated by a fixation cross, temporally (uniformly sampled duration of 500 - 1000s) and spatially jittered (around the centre of the screen in a radius of a 1 cm) to attenuate its potential usefulness as a reference point. After each illusion type block, an arbitrary score is presented (computed as a scaled Inverse Efficiency Score) as a gamification mechanism to increase motivation to perform to the best of one’s abilities. To mitigate for the potential variability in the speed/accuracy trade-off, the instructions emphasize with equal weight to be fast and to avoid errors.</p>
</div>
<p>For each illusion type, two continuous dimensions are orthogonally manipulated namely task difficulty and illusion strength, so that each trial corresponds to a unique combination, <strong>providing an objectively correct answer for each trial.</strong> <strong>The use of these manipulations allows concise, standardised reporting of illusion parameters and ensures our stimuli are fully reproducible <span class="citation" data-cites="makowski2021">(see <a href="#ref-makowski2021" role="doc-biblioref">Makowski et al., 2021</a>)</span>.</strong></p>
<p>Task difficulty corresponds to the difficulty of the perceptual decision (e.g., if the task is to select the longest red line, task difficulty corresponds to how the lines are objectively different). Illusion strength corresponds to the degree to which the illusion elements (e.g., the black arrow lines in Müller-Lyer) are interfering with the aforementioned task. Note that the illusion effect can be either “incongruent”, <strong>making the task more difficult by biasing perceptual decisions toward the incorrect response</strong> or “congruent”, <strong>making the task easier by biasing decisions toward the correct response (e.g., in the Müller-Lyer illusion, if the outwards-facing arrowheads are placed on the longer line, identifying which line is the longest becomes easier)</strong>. Participants respond with a key arrow (left vs.&nbsp;right; or up vs.&nbsp;down), and their reaction time (RT) and accuracy are recorded.</p>
<p>Visual illusion sensitivity will be measured as the average error rate in the incongruent condition, and separately for the 3 illusion types. Although the error rate is arguably a crude score, which does not take into account the effect of varying illusion strength, the interaction with task difficulty and the possible adjustments in response strategy (speed-accuracy trade off), it is also the most simple and easy to reproduce, hence its usage as our primary outcome for the current <strong>registered report</strong>. <strong>As a secondary exploratory outcome, the Inverse Efficiency Score <span class="citation" data-cites="townsend2014methods">(IES, <a href="#ref-townsend2014methods" role="doc-biblioref">Townsend &amp; Ashby, 2014</a>)</span> will also be computed. This metric incorporates both speed and accuracy by dividing the mean reaction time of correct responses by the proportion of correct responses, separately for each illusion.</strong></p>
<p>The two sets of 3 illusion blocks will be separated by 2 short questionnaires acting as a break, namely the IPIP-6 <span class="citation" data-cites="sibley2011mini">(<a href="#ref-sibley2011mini" role="doc-biblioref">Sibley et al., 2011</a>)</span>, measuring 6 personality traits with 24 analogue scales items, and the PID-5 <span class="citation" data-cites="krueger2011">(<a href="#ref-krueger2011" role="doc-biblioref">Krueger et al., 2011</a>)</span>, measuring 5 maladaptive personality traits with 25 Likert scales items. These questionnaires are included as a way of providing a break between the two cognitively taxing blocks and maintain paradigmatic consistency with previous studies <span class="citation" data-cites="makowski2023">(<a href="#ref-makowski2023" role="doc-biblioref">Makowski et al., 2023</a>)</span>. <strong>Additionally, the psychoticism subscale of the PID-5 will be used to examine the correlation between maladaptive traits and illusion sensitivity, evaluating the existence of the link proposed in previous studies <span class="citation" data-cites="costa2023">(<a href="#ref-costa2023" role="doc-biblioref">Costa et al., 2023</a>)</span>.</strong></p>
</section>
</section>
<section id="data-analysis" class="level2">
<h2 data-anchor-id="data-analysis">Data Analysis</h2>
<p><strong>The phenomenological control task will include several manipulation check indices to identify problematic participants.</strong> <strong>The task consists of various auditory and visual exercises; at the outset, participants hear a voice say “hello” and are asked to select the corresponding phrase from multiple options (e.g., “Hello,” “Goodbye,” “How are you,” “Thank you”).</strong> <strong>Selecting an incorrect response indicates inattention to auditory stimuli.</strong> <strong>In a subsequent exercise, participants are instructed: “Open your eyes. You will see only two balls on the screen… just two balls”.</strong> <strong>However, three differently coloured balls are displayed.</strong> <strong>If a participant selects the response “no balls were shown,” it suggests they failed to attend to both the auditory instruction and the visual stimuli.</strong> <strong>In another task, participants are instructed to press the spacebar six times.</strong> <strong>Pressing it fewer than five times within the allotted time indicates a failure to follow the auditory instructions.</strong> <strong>Participants who fail any one of these manipulation checks will be excluded from further analysis.</strong> <strong>Finally, reliability of the PCS will be assessed by computing Cronbach’s alpha <span class="citation" data-cites="cronbach1951coefficient">(<a href="#ref-cronbach1951coefficient" role="doc-biblioref">Cronbach, 1951</a>)</span>.</strong></p>
<p><strong>To assess whether the illusions functioned as expected, stimuli will be categorized into three groups: Strong Illusion Strength &amp; Incongruent, Mild Illusion Strength &amp; Incongruent, and Congruent.</strong> <strong>Two Bayesian t-tests will be conducted to assess differences in IES between the Congruent and Mild conditions, and between the Mild and Strong conditions.</strong> <strong>The IES is calculated by dividing the average correct RTs by the proportion of correct responses.</strong> <strong>Significant differences in these comparisons will provide evidence that the illusions functioned as intended.</strong></p>
<p><strong>The two outcome measures—error rate and IES—will be computed for each illusion and for each illusion strength group: Strong Illusion Strength &amp; Incongruent, Mild Illusion Strength &amp; Incongruent, and Congruent.</strong> <strong>Correlation will be computed between the mild and strong groups for each illusion and outcomes separately.</strong> <strong>If these correlations are high <span class="citation" data-cites="cohen2013statistical">(<em>r</em> &gt; .50, <a href="#ref-cohen2013statistical" role="doc-biblioref">Cohen, 2013</a>)</span>, the mild and strong illusion strength groups will be collapsed and the outcomes will be recomputed across all trials, otherwise they will be treated as separate in subsequent analyses.</strong></p>
<p><strong>Reliability analyses will then be conducted on all resulting indices.</strong> <strong>First, split-half reliability, to assess internal consistency, will be computed by correlating two equal subsets of individual scores, with high correlations expected (<em>r</em> &gt; .5).</strong> <strong>Second, inter-illusion reliability will be evaluated using Cronbach’s alpha across the three illusions.</strong></p>
<p>Illusion Game outliers will be flagged based on their RT distributions, following the same procedure as in <span class="citation" data-cites="makowski2023">(<a href="#ref-makowski2023" role="doc-biblioref">Makowski et al., 2023</a>)</span>. <strong>If the RT is collapsed to the left (i.e., has &gt; 1/3 of ultra-fast responses - typically &lt; 200 ms) in the first set, the entire participant will be discarded (suggesting that they did not properly do the task), but if only the second set is bad, then only the second set will be discarded (as the illusion sensitivity can still be estimated, albeit with less precision).</strong> In addition, the removal of individual trials will also be performed [RT &lt; 200 ms or &gt; 3 SD; following Thériault et al. (<a href="#ref-theriault2024">2024</a>)]. <strong>To mitigate the risk of confounding effects driven by extreme speed or accuracy strategies, participants whose RTs are significantly slower than the group average <span class="citation" data-cites="makowski2023">(RT &gt; 4 SD above the mean, based on <a href="#ref-makowski2023" role="doc-biblioref">Makowski et al., 2023</a>)</span> will be excluded from the analysis.</strong></p>
<p>After removing problematic participants and trials, the outcome measures (PC and VI sensitivity scores) will be computed and the Bayesian correlation (with medium prior on the coefficient, i.e., r-scale parameter set to 1/3) will be computed <span class="citation" data-cites="BayesFactor">(using the <em>BayesFactor</em> package, <a href="#ref-BayesFactor" role="doc-biblioref">Morey &amp; Rouder, 2024</a>)</span>. <strong>This shifted beta prior, as recommended by Morey and Rouder (<a href="#ref-morey2018baysefactor">2018</a>), offers a balanced approach to estimating effect sizes, without placing undue weight on larger effect sizes or artificially inflating evidence for the null hypothesis.</strong> Following Lush et al. (<a href="#ref-lush2022">2022</a>), we expect to collect evidence against (BF10 &lt;= 1/3) a relationship between PCS and VI sensitivity. <strong>Additionally, Bayesian correlations will be computed using the BayesFactor package, employing a medium prior on the coefficient (r-scale parameter set to 1/3) to assess relationships between maladaptive trait facets and illusion sensitivity scores.</strong> <strong>Based on prior research (Makowski et al., 2023), we expect to find evidence (BF10 ≥ 3) supporting a relationship between the psychoticism facet of the PID-5 and illusion sensitivity.</strong> Data analysis will be carried out using R, using <em>tidyverse</em> <span class="citation" data-cites="tidyverse">(<a href="#ref-tidyverse" role="doc-biblioref">Wickham et al., 2019</a>)</span> and <em>easystats</em> <span class="citation" data-cites="easystats parameters correlation bayestestR datawizard">(<a href="#ref-parameters" role="doc-biblioref">Lüdecke et al., 2020</a>, <a href="#ref-easystats" role="doc-biblioref">2022</a>; <a href="#ref-bayestestR" role="doc-biblioref">Makowski et al., 2019</a>, <a href="#ref-correlation" role="doc-biblioref">2022</a>; <a href="#ref-datawizard" role="doc-biblioref">Patil et al., 2022</a>)</span>. The analysis script and additional information are available at <strong>https://osf.io/da3u6/?view_only=247d4efa1afe456aa07662732946d4e6</strong> [Note this link will be replaced with the GitHub page of the current project upon completion of the review process to ensure continued anonymisation].</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p><em>This section will be completed after data is collected.</em></p>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p><em>This section will be completed after data is collected.</em></p>
</section>
<section id="data-availability" class="level1">
<h1>Data Availability</h1>
<p>All the study materials, experiment, data, and analysis is available on GitHub. [For the review process the pcs materials, the illusion game, and the analyses scripts can be accessed here: <strong>https://osf.io/da3u6/?view_only=247d4efa1afe456aa07662732946d4e6</strong>. Note this link will be replaced with the GitHub page of the current project upon completion of the review process to ensure continued anonymisation].</p>
</section>
<section id="acknowledgments" class="level1">
<h1>Acknowledgments</h1>
<p>We would like to thank An Shu Te for her help in setting up the project, Ryan Scott for his help in implementing the phenomenological control scale, and Zoltan Dienes for his input, feedback and guidance.</p>
<div style="page-break-after: always;"></div>
</section>
<section id="references" class="level1">
<h1>References</h1>
<!-- References will auto-populate in the refs div below -->
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-cohen2013statistical" class="csl-entry" role="listitem">
Cohen, J. (2013). <em>Statistical power analysis for the behavioral sciences</em>. routledge.
</div>
<div id="ref-costa2023" class="csl-entry" role="listitem">
Costa, A. L. L., Costa, D. L., Pessoa, V. F., Caixeta, F. V., &amp; Maior, R. S. (2023). Systematic review of visual illusions in schizophrenia. <em>Schizophrenia Research</em>, <em>252</em>, 13–22. <a href="https://doi.org/10.1016/j.schres.2022.12.030">https://doi.org/10.1016/j.schres.2022.12.030</a>
</div>
<div id="ref-cronbach1951coefficient" class="csl-entry" role="listitem">
Cronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. <em>Psychometrika</em>, <em>16</em>(3), 297–334.
</div>
<div id="ref-de2015jspsych" class="csl-entry" role="listitem">
De Leeuw, J. R. (2015). jsPsych: A JavaScript library for creating behavioral experiments in a web browser. <em>Behavior Research Methods</em>, <em>47</em>, 1–12.
</div>
<div id="ref-deleeuw2023" class="csl-entry" role="listitem">
De Leeuw, J. R. (2023). DataPipe: Born-open data collection for online experiments. <em>Behavior Research Methods</em>, <em>56</em>(3), 2499–2506. <a href="https://doi.org/10.3758/s13428-023-02161-x">https://doi.org/10.3758/s13428-023-02161-x</a>
</div>
<div id="ref-dienes2022" class="csl-entry" role="listitem">
Dienes, Z., Lush, P., Palfi, B., Roseboom, W., Scott, R., Parris, B., Seth, A., &amp; Lovell, M. (2022). Phenomenological control as cold control. <em>Psychology of Consciousness: Theory, Research, and Practice</em>, <em>9</em>(2), 101–116. <a href="https://doi.org/10.1037/cns0000230">https://doi.org/10.1037/cns0000230</a>
</div>
<div id="ref-friston2010" class="csl-entry" role="listitem">
Friston, K. (2010). The free-energy principle: a unified brain theory? <em>Nature Reviews Neuroscience</em>, <em>11</em>(2), 127–138. <a href="https://doi.org/10.1038/nrn2787">https://doi.org/10.1038/nrn2787</a>
</div>
<div id="ref-gershman2012" class="csl-entry" role="listitem">
Gershman, S. J., Vul, E., &amp; Tenenbaum, J. B. (2012). Multistability and Perceptual Inference. <em>Neural Computation</em>, <em>24</em>(1), 1–24. <a href="https://doi.org/10.1162/neco_a_00226">https://doi.org/10.1162/neco_a_00226</a>
</div>
<div id="ref-gori2016" class="csl-entry" role="listitem">
Gori, S., Molteni, M., &amp; Facoetti, A. (2016). Visual illusions: An interesting tool to investigate developmental dyslexia and autism spectrum disorder. <em>Frontiers in Human Neuroscience</em>, <em>10</em>. <a href="https://doi.org/10.3389/fnhum.2016.00175">https://doi.org/10.3389/fnhum.2016.00175</a>
</div>
<div id="ref-krueger2011" class="csl-entry" role="listitem">
Krueger, R. F., Eaton, N. R., Derringer, J., Markon, K. E., Watson, D., &amp; Skodol, A. E. (2011). Personality in<span><em>DSM<span></span>5:</em></span>Helping Delineate Personality Disorder Content and Framing the Metastructure. <em>Journal of Personality Assessment</em>, <em>93</em>(4), 325–331. <a href="https://doi.org/10.1080/00223891.2011.577478">https://doi.org/10.1080/00223891.2011.577478</a>
</div>
<div id="ref-parameters" class="csl-entry" role="listitem">
Lüdecke, D., Ben-Shachar, M. S., Patil, I., &amp; Makowski, D. (2020). <em>Extracting, computing and exploring the parameters of statistical models using <span></span>r<span></span>.</em> <em>5</em>, 2445. <a href="https://doi.org/10.21105/joss.02445">https://doi.org/10.21105/joss.02445</a>
</div>
<div id="ref-easystats" class="csl-entry" role="listitem">
Lüdecke, D., Ben-Shachar, M. S., Patil, I., Wiernik, B. M., Bacher, E., Thériault, R., &amp; Makowski, D. (2022). <em>Easystats: Framework for easy statistical modeling, visualization, and reporting</em>. <a href="https://easystats.github.io/easystats/">https://easystats.github.io/easystats/</a>
</div>
<div id="ref-lush2018" class="csl-entry" role="listitem">
Lush, P., Moga, G., McLatchie, N., &amp; Dienes, Z. (2018). The Sussex-Waterloo Scale of Hypnotizability (SWASH): measuring capacity for altering conscious experience. <em>Neuroscience of Consciousness</em>, <em>2018</em>(1). <a href="https://doi.org/10.1093/nc/niy006">https://doi.org/10.1093/nc/niy006</a>
</div>
<div id="ref-lush2021" class="csl-entry" role="listitem">
Lush, P., Scott, R. B., Seth, A. K., &amp; Dienes, Z. (2021). The Phenomenological Control Scale: Measuring the Capacity for Creating Illusory Nonvolition, Hallucination and Delusion. <em>Collabra: Psychology</em>, <em>7</em>(1). <a href="https://doi.org/10.1525/collabra.29542">https://doi.org/10.1525/collabra.29542</a>
</div>
<div id="ref-lush2022" class="csl-entry" role="listitem">
Lush, P., Seth, A., Dienes, Z., &amp; Scott, R. B. (2022). <em>Trait phenomenological control in top-down and bottom-up effects: ASMR, visually evoked auditory response and the müller-lyer illusion</em>. <a href="http://dx.doi.org/10.31234/osf.io/hw4y9">http://dx.doi.org/10.31234/osf.io/hw4y9</a>
</div>
<div id="ref-bayestestR" class="csl-entry" role="listitem">
Makowski, D., Ben-Shachar, M. S., &amp; Lüdecke, D. (2019). <em>bayestestR: Describing effects and their uncertainty, existence and significance within the bayesian framework.</em> <em>4</em>, 1541. <a href="https://doi.org/10.21105/joss.01541">https://doi.org/10.21105/joss.01541</a>
</div>
<div id="ref-makowski2021" class="csl-entry" role="listitem">
Makowski, D., Lau, Z. J., Pham, T., Paul Boyce, W., &amp; Annabel Chen, S. H. (2021). A Parametric Framework to Generate Visual Illusions Using Python. <em>Perception</em>, <em>50</em>(11), 950–965. <a href="https://doi.org/10.1177/03010066211057347">https://doi.org/10.1177/03010066211057347</a>
</div>
<div id="ref-makowski2023" class="csl-entry" role="listitem">
Makowski, D., Te, A. S., Kirk, S., Liang, N. Z., &amp; Chen, S. H. A. (2023). A novel visual illusion paradigm provides evidence for a general factor of illusion sensitivity and personality correlates. <em>Scientific Reports</em>, <em>13</em>(1). <a href="https://doi.org/10.1038/s41598-023-33148-5">https://doi.org/10.1038/s41598-023-33148-5</a>
</div>
<div id="ref-correlation" class="csl-entry" role="listitem">
Makowski, D., Wiernik, B. M., Patil, I., Lüdecke, D., &amp; Ben-Shachar, M. S. (2022). <em><span></span><span></span>Correlation<span></span><span></span>: Methods for correlation analysis</em>. <a href="https://CRAN.R-project.org/package=correlation">https://CRAN.R-project.org/package=correlation</a>
</div>
<div id="ref-mitchell2010" class="csl-entry" role="listitem">
Mitchell, P., Mottron, L., Soulières, I., &amp; Ropar, D. (2010). Susceptibility to the Shepard illusion in participants with autism: reduced top<span>-</span>down influences within perception? <em>Autism Research</em>, <em>3</em>(3), 113–119. <a href="https://doi.org/10.1002/aur.130">https://doi.org/10.1002/aur.130</a>
</div>
<div id="ref-morey2018baysefactor" class="csl-entry" role="listitem">
Morey, R. D., &amp; Rouder, J. N. (2018). <em>BayseFactor: Computation of bayes factors for common designs</em>.
</div>
<div id="ref-BayesFactor" class="csl-entry" role="listitem">
Morey, R. D., &amp; Rouder, J. N. (2024). <em>BayesFactor: Computation of bayes factors for common designs</em>. <a href="https://CRAN.R-project.org/package=BayesFactor">https://CRAN.R-project.org/package=BayesFactor</a>
</div>
<div id="ref-notredame2014" class="csl-entry" role="listitem">
Notredame, C.-E., Pins, D., Deneve, S., &amp; Jardri, R. (2014). What visual illusions teach us about schizophrenia. <em>Frontiers in Integrative Neuroscience</em>, <em>8</em>. <a href="https://doi.org/10.3389/fnint.2014.00063">https://doi.org/10.3389/fnint.2014.00063</a>
</div>
<div id="ref-nour2015perception" class="csl-entry" role="listitem">
Nour, M. M., &amp; Nour, J. M. (2015). Perception, illusions and bayesian inference. <em>Psychopathology</em>, <em>48</em>(4), 217–221.
</div>
<div id="ref-palmer2017" class="csl-entry" role="listitem">
Palmer, C. J., Lawson, R. P., &amp; Hohwy, J. (2017). Bayesian approaches to autism: Towards volatility, action, and behavior. <em>Psychological Bulletin</em>, <em>143</em>(5), 521–542. <a href="https://doi.org/10.1037/bul0000097">https://doi.org/10.1037/bul0000097</a>
</div>
<div id="ref-datawizard" class="csl-entry" role="listitem">
Patil, I., Makowski, D., Ben-Shachar, M. S., Wiernik, B. M., Bacher, E., &amp; Lüdecke, D. (2022). <em><span></span>Datawizard<span></span>: An <span></span>r<span></span> package for easy data preparation and statistical transformations</em>. <em>7</em>, 4684. <a href="https://doi.org/10.21105/joss.04684">https://doi.org/10.21105/joss.04684</a>
</div>
<div id="ref-shoshina2014" class="csl-entry" role="listitem">
Shoshina, I. I., &amp; Shelepin, Yu. E. (2014). Effectiveness of Discrimination of the Sizes of Line Segments by Humans with Different Cognitive Style Parameters. <em>Neuroscience and Behavioral Physiology</em>, <em>44</em>(7), 748–753. <a href="https://doi.org/10.1007/s11055-014-9978-2">https://doi.org/10.1007/s11055-014-9978-2</a>
</div>
<div id="ref-sibley2011mini" class="csl-entry" role="listitem">
Sibley, C. G., Luyten, N., Purnomo, M., Mobberley, A., Wootton, L. W., Hammond, M. D., Sengupta, N., Perry, R., West-Newman, T., Wilson, M. S., et al. (2011). The mini-IPIP6: Validation and extension of a short measure of the big-six factors of personality in new zealand. <em>New Zealand Journal of Psychology</em>, <em>40</em>(3).
</div>
<div id="ref-sundareswara2008" class="csl-entry" role="listitem">
Sundareswara, R., &amp; Schrater, P. R. (2008). Perceptual multistability predicted by search model for Bayesian decisions. <em>Journal of Vision</em>, <em>8</em>(5), 12. <a href="https://doi.org/10.1167/8.5.12">https://doi.org/10.1167/8.5.12</a>
</div>
<div id="ref-theriault2024" class="csl-entry" role="listitem">
Thériault, R., Ben-Shachar, M. S., Patil, I., Lüdecke, D., Wiernik, B. M., &amp; Makowski, D. (2024). Check your outliers﻿! An introduction to identifying statistical outliers in R with easystats. <em>Behavior Research Methods</em>. <a href="https://doi.org/10.3758/s13428-024-02356-w">https://doi.org/10.3758/s13428-024-02356-w</a>
</div>
<div id="ref-todorovic2020" class="csl-entry" role="listitem">
Todorović, D. (2020). What Are Visual Illusions? <em>Perception</em>, <em>49</em>(11), 1128–1199. <a href="https://doi.org/10.1177/0301006620962279">https://doi.org/10.1177/0301006620962279</a>
</div>
<div id="ref-townsend2014methods" class="csl-entry" role="listitem">
Townsend, J. T., &amp; Ashby, F. G. (2014). Methods of modeling capacity in simple processing systems. In <em>Cognitive theory</em> (pp. 199–239). Psychology Press.
</div>
<div id="ref-tidyverse" class="csl-entry" role="listitem">
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). <em>Welcome to the <span></span>tidyverse<span></span></em>. <em>4</em>, 1686. <a href="https://doi.org/10.21105/joss.01686">https://doi.org/10.21105/joss.01686</a>
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>